
@incollection{kamm_use_2020,
	title = {The {Use} and {Abuse} of the {Trolley} {Problem}: {Self}-{Driving} {Cars}, {Medical} {Treatments}, and the {Distribution} of {Harm}},
	isbn = {978-0-19-090503-3 978-0-19-090507-1},
	shorttitle = {The {Use} and {Abuse} of the {Trolley} {Problem}},
	url = {https://academic.oup.com/book/33540/chapter/287904581},
	abstract = {This chapter begins with a brief overview of what are known as standard Trolley Problem Cases. Next, it points out that many cases that are presented as Trolley Problem Cases in the AI ethics literature in fact raise moral issues distinctive from the issues raised by standard Trolley Problem Cases. This chapter then critically examines some discussions that seek to apply the Trolley Problem to choices involved in self-driving cars, the use of e-cigarettes, and mammography. The final section considers conceptual and moral issues involved in the production and use of self-driving cars, such as the role and responsibility of those who program such cars, the liability of pedestrians and drivers to be harmed by such cars, and whether voluntary passengers of self-driving cars are even more liable to be harmed than pedestrians.},
	language = {en},
	urldate = {2024-01-30},
	booktitle = {Ethics of {Artificial} {Intelligence}},
	publisher = {Oxford University Press},
	author = {Kamm, F. M.},
	collaborator = {Kamm, F. M.},
	month = sep,
	year = {2020},
	doi = {10.1093/oso/9780190905033.003.0003},
	pages = {79--108},
}

@incollection{dubber_perspectives_2020,
	title = {Perspectives on {Ethics} of {AI}: {Computer} {Science}},
	isbn = {978-0-19-006739-7},
	shorttitle = {Perspectives on {Ethics} of {AI}},
	url = {https://academic.oup.com/edited-volume/34287/chapter/290666837},
	abstract = {This chapter describes a computational view of the function of ethics in human society and discusses its application to three diverse examples. First, autonomous vehicles are individually embodied intelligent systems that act as members of society. The ethical knowledge needed by such an agent is not how to choose the lesser evil when confronted by a Deadly Dilemma, but how to recognize the upstream decision point that makes it possible to avoid the Deadly Dilemma entirely. Second, disembodied distributed intelligent systems like Google and Facebook provide valuable services while collecting, aggregating, and correlating vast amounts of information about individual users. With inadequate controls, these corporate systems can invade privacy and do substantial damage through either correct or incorrect inferences. Third, acceptance of the legitimacy of the society by its individual members depends on a general perception of fairness. Rage about unfairness can be directed at individual free-riders or at systematic inequality across the society. Ultimately, the promise of a computational approach to ethical knowledge is not simply ethics for computational devices such as robots. It also promises to help people understand the pragmatic value of ethics as a feedback mechanism that helps intelligent creatures, human and nonhuman, live together in thriving societies.},
	language = {en},
	urldate = {2024-01-30},
	booktitle = {The {Oxford} {Handbook} of {Ethics} of {AI}},
	publisher = {Oxford University Press},
	author = {Kuipers, Benjamin},
	editor = {Dubber, Markus D. and Pasquale, Frank and Das, Sunit},
	collaborator = {Kuipers, Benjamin},
	month = jul,
	year = {2020},
	doi = {10.1093/oxfordhb/9780190067397.013.27},
	pages = {419--441},
}

@misc{cornell_phd_18_2023,
	title = {18 {Moral} {Dilemma} {Examples} (2023)},
	url = {https://helpfulprofessor.com/moral-dilemma-examples/},
	abstract = {A moral dilemma is a situation in which an individual must choose between two moral options. Each option has advantages and disadvantages that contain},
	language = {en-US},
	urldate = {2023-12-16},
	author = {Cornell (PhD), Dave and Drew (PhD), Chris},
	month = jul,
	year = {2023},
}

@article{awad_universals_2020,
	title = {Universals and variations in moral decisions made in 42 countries by 70,000 participants},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1911517117},
	doi = {10.1073/pnas.1911517117},
	abstract = {When do people find it acceptable to sacrifice one life to save many? Cross-cultural studies suggested a complex pattern of universals and variations in the way people approach this question, but data were often based on small samples from a small number of countries outside of the Western world. Here we analyze responses to three sacrificial dilemmas by 70,000 participants in 10 languages and 42 countries. In every country, the three dilemmas displayed the same qualitative ordering of sacrifice acceptability, suggesting that this ordering is best explained by basic cognitive processes rather than cultural norms. The quantitative acceptability of each sacrifice, however, showed substantial country-level variations. We show that low relational mobility (where people are more cautious about not alienating their current social partners) is strongly associated with the rejection of sacrifices for the greater good (especially for Eastern countries), which may be explained by the signaling value of this rejection. We make our dataset fully available as a public resource for researchers studying universals and variations in human morality.},
	language = {en},
	number = {5},
	urldate = {2023-12-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Awad, Edmond and Dsouza, Sohan and Shariff, Azim and Rahwan, Iyad and Bonnefon, Jean-François},
	month = feb,
	year = {2020},
	pages = {2332--2337},
}

@misc{lourie_scruples_2021,
	title = {Scruples: {A} {Corpus} of {Community} {Ethical} {Judgments} on 32,000 {Real}-{Life} {Anecdotes}},
	shorttitle = {Scruples},
	url = {http://arxiv.org/abs/2008.09094},
	doi = {10.48550/arXiv.2008.09094},
	abstract = {As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics. We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks. A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Lourie, Nicholas and Bras, Ronan Le and Choi, Yejin},
	month = mar,
	year = {2021},
	note = {arXiv:2008.09094 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@incollection{railton_ethical_2020,
	title = {Ethical {Learning}, {Natural} and {Artificial}},
	isbn = {978-0-19-090503-3 978-0-19-090507-1},
	url = {https://academic.oup.com/book/33540/chapter/287904390},
	abstract = {How might artificial systems become sensitive to ethically relevant considerations? This chapter argues that the question we should ask is not “How can we build ethics into robots?” but rather “How can we build robots with a capacity for ethical learning?” It is evident from the continuing disagreement over overarching ethical theories that we do not know a set of consensus ethical principles with sufficient definiteness to “program” ethics into a machine. Recent research in artificial intelligence has shown the power of general-purpose learning to acquire autonomous human-level competencies. Intriguingly, recent research in developmental psychology suggests that children may also acquire independent social and ethical competence via similar mechanisms. Instead of programing ethics into robots, this chapter proposes that artificial systems could gain ethical competence through general-purpose learning, comparable to infants learning human ethics in part by observing adult behavior.},
	language = {en},
	urldate = {2024-01-30},
	booktitle = {Ethics of {Artificial} {Intelligence}},
	publisher = {Oxford University Press},
	author = {Railton, Peter},
	collaborator = {Railton, Peter},
	month = sep,
	year = {2020},
	doi = {10.1093/oso/9780190905033.003.0002},
	pages = {45--78},
}

@incollection{mcconnell_moral_2022,
	edition = {Fall 2022},
	title = {Moral {Dilemmas}},
	url = {https://plato.stanford.edu/archives/fall2022/entries/moral-dilemmas/},
	abstract = {Moral dilemmas, at the very least, involve conflicts between moralrequirements. Consider the cases given below.},
	urldate = {2024-01-30},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {McConnell, Terrance},
	editor = {Zalta, Edward N. and Nodelman, Uri},
	year = {2022},
	keywords = {dirty hands, the problem of, Kant, Immanuel, logic: deontic, Mill, John Stuart, Plato, Sartre, Jean-Paul},
}

@misc{ai_learn_nodate,
	title = {Learn {Prompting} 101: {Prompt} {Engineering} {Course} \& {Challenges} – {Towards} {AI}},
	shorttitle = {Learn {Prompting} 101},
	url = {https://towardsai.net/p/machine-learning/learn-prompting-101-prompt-engineering-course, https://towardsai.net/p/machine-learning/learn-prompting-101-prompt-engineering-course},
	abstract = {Introduction. The capabilities and accessibility of large language models (LLMs) are advancing rapidly, leading to widespread adoption and increasing human-AI interaction...},
	language = {en-US},
	urldate = {2024-01-30},
	author = {AI, Towards},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	doi = {10.48550/arXiv.2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{zhou_large_2023,
	title = {Large {Language} {Models} {Are} {Human}-{Level} {Prompt} {Engineers}},
	url = {http://arxiv.org/abs/2211.01910},
	doi = {10.48550/arXiv.2211.01910},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = mar,
	year = {2023},
	note = {arXiv:2211.01910 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
