{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-07T19:47:32.038027Z",
     "iopub.status.busy": "2024-03-07T19:47:32.037683Z",
     "iopub.status.idle": "2024-03-07T19:47:39.383623Z",
     "shell.execute_reply": "2024-03-07T19:47:39.382814Z",
     "shell.execute_reply.started": "2024-03-07T19:47:32.038000Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T19:48:18.652837Z",
     "iopub.status.busy": "2024-03-07T19:48:18.652194Z",
     "iopub.status.idle": "2024-03-07T19:48:18.777796Z",
     "shell.execute_reply": "2024-03-07T19:48:18.776936Z",
     "shell.execute_reply.started": "2024-03-07T19:48:18.652804Z"
    }
   },
   "outputs": [],
   "source": [
    "df_work = pd.read_csv(\"/kaggle/input/falcon-wip/filtered_runs46_100.csv\")\n",
    "df_middle = pd.read_csv(\"/kaggle/input/falcon-wip/falcon_results_clean_run13_5.csv\")\n",
    "df_prompts = pd.read_csv(\"/kaggle/input/prompts-all-llms/prompts.csv\")\n",
    "df = df_prompts.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and initializing quantized Falcon 7b instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T19:48:20.424440Z",
     "iopub.status.busy": "2024-03-07T19:48:20.424065Z",
     "iopub.status.idle": "2024-03-07T19:49:41.183497Z",
     "shell.execute_reply": "2024-03-07T19:49:41.182391Z",
     "shell.execute_reply.started": "2024-03-07T19:48:20.424411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3dc8ac5ee840a28afa057d1606b8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3973fa82d2645068ad3aa9878883e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f906d2ed452f4d66a5e68645af3b682a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 19:48:23.609149: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 19:48:23.609280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 19:48:23.779501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acb47a80fd642edaa66327bc569f885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201587a5af5542b987875882130dad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9802817e61449d9493a3139e4b2478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5c2f84303f4ddc98085de7b6abed17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b76f8ea23641d2b4f009d301ea2ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf34d8c11a3a4301a4c06ee30a540d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c669955f0414467b440be17c2208194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b605b1e30e43ee9ba6eddcf20c2684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e3f30f087d4ed0866258e96b0b3b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T19:49:52.190795Z",
     "iopub.status.busy": "2024-03-07T19:49:52.190448Z",
     "iopub.status.idle": "2024-03-07T19:59:49.888427Z",
     "shell.execute_reply": "2024-03-07T19:59:49.887530Z",
     "shell.execute_reply.started": "2024-03-07T19:49:52.190769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# initialize empty lists to store generated texts for each run\n",
    "generated_texts = [[] for _ in range(0,101)]\n",
    "\n",
    "for run in range(0, 101):\n",
    "    for index, row in df.iterrows():\n",
    "        # obtain prompt for current row\n",
    "        prompt = row['Prompt']\n",
    "\n",
    "        # generate text\n",
    "        sequences = pipeline(\n",
    "            prompt,\n",
    "            max_length=500,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # extract the generated text from the sequences\n",
    "        for seq in sequences:\n",
    "            generated_text = seq['generated_text']\n",
    "            \n",
    "            generated_texts[run].append(generated_text)\n",
    "\n",
    "# add each list of generated texts to df as new column\n",
    "for run in range(0, 101):\n",
    "    df[f'run_{run+1}'] = generated_texts[run]\n",
    "    df.to_csv(\"working.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T19:59:54.127489Z",
     "iopub.status.busy": "2024-03-07T19:59:54.126701Z",
     "iopub.status.idle": "2024-03-07T19:59:54.140946Z",
     "shell.execute_reply": "2024-03-07T19:59:54.140039Z",
     "shell.execute_reply.started": "2024-03-07T19:59:54.127451Z"
    }
   },
   "outputs": [],
   "source": [
    "# save initial results to csv\n",
    "df_prompts.to_csv(\"falcon_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that after we had changed our prompts, Mistral was performing very well and Falcon struggeled to return any type of answer and often just gave a rephrased version of our prompt rather than a proper response. I thus pre-filtered every cell of my results-dataframe and deleted any type of string matching our initial prompt. The empty cells are then filled by iterating over the dataframe and checking, if new text has to be generated.\n",
    "\n",
    "Upon observing the count of empty cells, I realized that about 50% of responses after every Falcon 7B run were 'empty'. I stopped iterating, once I obtained valid responses for all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:00:05.595363Z",
     "iopub.status.busy": "2024-03-07T20:00:05.594504Z",
     "iopub.status.idle": "2024-03-07T20:00:05.601501Z",
     "shell.execute_reply": "2024-03-07T20:00:05.600543Z",
     "shell.execute_reply.started": "2024-03-07T20:00:05.595334Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_responses(df, prompt_column, response_columns):\n",
    "    for col in response_columns:\n",
    "        filtered_responses = []\n",
    "        for idx, row in df.iterrows():\n",
    "            prompt = row[prompt_column]\n",
    "            response = row[col]\n",
    "            # Using regular expressions to match the start of the response with the prompt\n",
    "            match = re.match(r'^{}[\\s:]*'.format(re.escape(prompt)), response)\n",
    "            if match:\n",
    "                filtered_responses.append(response[match.end():].strip())\n",
    "            else:\n",
    "                filtered_responses.append(response)  # If no match, keep the original response\n",
    "        df['filtered_' + col] = filtered_responses\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:01:13.628342Z",
     "iopub.status.busy": "2024-03-07T20:01:13.627948Z",
     "iopub.status.idle": "2024-03-07T20:01:13.633924Z",
     "shell.execute_reply": "2024-03-07T20:01:13.632890Z",
     "shell.execute_reply.started": "2024-03-07T20:01:13.628314Z"
    }
   },
   "outputs": [],
   "source": [
    "# obtain list of column names for filtering - note that I used some test runs to check my results\n",
    "col_strings_list = ['run_' + str(i) for i in range(6, 46)]\n",
    "col_strings_list_long = ['run_' + str(i) for i in range(46, 101)]\n",
    "col_strings_short = ['run_' + str(i) for i in range(1, 6)]\n",
    "col_strings_full = ['run_' + str(i) for i in range (1, 101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:01:19.924746Z",
     "iopub.status.busy": "2024-03-07T20:01:19.924351Z",
     "iopub.status.idle": "2024-03-07T20:01:20.048879Z",
     "shell.execute_reply": "2024-03-07T20:01:20.048034Z",
     "shell.execute_reply.started": "2024-03-07T20:01:19.924716Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter original data\n",
    "#df_middle = filter_responses(df_middle, 'Prompt', col_strings_list)\n",
    "#df_sample = filter_responses(df_sample, 'Prompt', col_strings_list_long)\n",
    "#df_prompts = filter_responses(df_prompts, 'Prompt', col_strings_short)\n",
    "df = filter_responses(df, 'Prompt', col_strings_full)\n",
    "\n",
    "# save filtered version to csv file\n",
    "#df_middle.to_csv('filtered_responses_6till45.csv', index=False)\n",
    "#df_prompts.to_csv('filtered_runs_5_46.csv', index=False)\n",
    "df.to_csv('filtered_runs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:13:30.860702Z",
     "iopub.status.busy": "2024-03-07T20:13:30.860325Z",
     "iopub.status.idle": "2024-03-07T20:13:38.606151Z",
     "shell.execute_reply": "2024-03-07T20:13:38.605389Z",
     "shell.execute_reply.started": "2024-03-07T20:13:30.860671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    for col_name in col_strings_full:\n",
    "        filtered_col_name = 'filtered_' + col_name\n",
    "        if pd.isna(row[filtered_col_name]) or row[filtered_col_name] == '':\n",
    "            prompt = row['Prompt']\n",
    "            \n",
    "            # generate answer using the prompt\n",
    "            sequences = pipeline(\n",
    "                prompt,\n",
    "                max_length=500,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            # obtain answer from sequences and filter out prompt\n",
    "            for seq in sequences:\n",
    "                generated_text = seq['generated_text']\n",
    "                \n",
    "            if generated_text:\n",
    "                # update the corresponding cell with newly generated text\n",
    "                df.loc[idx, col_name] = generated_text\n",
    "            else:\n",
    "                # handle exception when generated text is empty\n",
    "                print(\"Generated text is empty for column:\", col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:08:17.331642Z",
     "iopub.status.busy": "2024-03-07T20:08:17.330851Z",
     "iopub.status.idle": "2024-03-07T20:08:17.347335Z",
     "shell.execute_reply": "2024-03-07T20:08:17.346528Z",
     "shell.execute_reply.started": "2024-03-07T20:08:17.331609Z"
    }
   },
   "outputs": [],
   "source": [
    "df_prompts.to_csv(\"Falcon_working_version1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we start to clean and to iterate and do it over and over again... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:14:05.874410Z",
     "iopub.status.busy": "2024-03-07T20:14:05.874041Z",
     "iopub.status.idle": "2024-03-07T20:14:05.914866Z",
     "shell.execute_reply": "2024-03-07T20:14:05.913807Z",
     "shell.execute_reply.started": "2024-03-07T20:14:05.874381Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter original data\n",
    "df = filter_responses(df, 'Prompt', col_strings_full)\n",
    "\n",
    "# save filtered version to csv file\n",
    "df.to_csv('filtered_working_version1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T20:14:08.162088Z",
     "iopub.status.busy": "2024-03-07T20:14:08.161722Z",
     "iopub.status.idle": "2024-03-07T20:14:08.172277Z",
     "shell.execute_reply": "2024-03-07T20:14:08.171257Z",
     "shell.execute_reply.started": "2024-03-07T20:14:08.162059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of empty strings in the DataFrame: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count empty strings and NaNs in dataframe\n",
    "empty_string_count = (df == '').sum().sum()\n",
    "print(\"Count of empty strings in the DataFrame:\", empty_string_count)\n",
    "\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T18:40:47.440700Z",
     "iopub.status.busy": "2024-03-07T18:40:47.439683Z",
     "iopub.status.idle": "2024-03-07T18:40:47.538255Z",
     "shell.execute_reply": "2024-03-07T18:40:47.537249Z",
     "shell.execute_reply.started": "2024-03-07T18:40:47.440638Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('falcon_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4483957,
     "sourceId": 7758222,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4538620,
     "sourceId": 7785280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
